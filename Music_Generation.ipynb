{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from music21 import*\n",
    "from glob import glob\n",
    "import IPython\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = glob('C://Users//ishik//Desktop//Data//new/*.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    notes = []\n",
    "    for file in songs:\n",
    "        # converting .mid file to stream object\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse = []\n",
    "        try:\n",
    "            # Given a single stream, partition into a part for each unique instrument\n",
    "            parts = instrument.partitionByInstrument(midi)\n",
    "        except:\n",
    "            pass\n",
    "        if parts: # if parts has instrument parts \n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else:\n",
    "            notes_to_parse = midi.flat.notes\n",
    "    \n",
    "        for element in notes_to_parse: \n",
    "            if isinstance(element, note.Note):\n",
    "                # if element is a note, extract pitch\n",
    "                notes.append(str(element.pitch))\n",
    "            elif(isinstance(element, chord.Chord)):\n",
    "                # if element is a chord, append the normal form of the \n",
    "                # chord (a list of integers) to the list of notes. \n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    with open('C://Users//ishik//Desktop//Data//notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab): \n",
    "    sequence_length = 100\n",
    "\n",
    "    # Extract the unique pitches in the list of notes.\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i: i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "    \n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    # reshape the input into a format comatible with LSTM layers \n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    \n",
    "    # one hot encode the output vectors\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    \n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Dropout, Flatten\n",
    "def create_network(network_in, n_vocab): \n",
    "    \"\"\"Create the model architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=network_in.shape[1:], return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "def train(model, network_input, network_output, epochs): \n",
    "    \"\"\"\n",
    "    Train the neural network\n",
    "    \"\"\"\n",
    "    # Create checkpoint to save the best model weights.\n",
    "    filepath = 'weights.best.music3.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True)\n",
    "    \n",
    "    model.fit(network_input, network_output, epochs=epochs, batch_size=32, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network():\n",
    "    \"\"\"\n",
    "    Get notes\n",
    "    Generates input and output sequences\n",
    "    Creates a model \n",
    "    Trains the model for the given epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    epochs = 50\n",
    "    \n",
    "    notes = get_notes()\n",
    "    print('Notes processed')\n",
    "    \n",
    "    n_vocab = len(set(notes))\n",
    "    print('Vocab generated')\n",
    "    \n",
    "    network_in, network_out = prepare_sequences(notes, n_vocab)\n",
    "    print('Input and Output processed')\n",
    "    \n",
    "    model = create_network(network_in, n_vocab)\n",
    "    print('Model created')\n",
    "    print('Training in progress')\n",
    "    train(model, network_in, network_out, epochs)\n",
    "    print('Training completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes processed\n",
      "Vocab generated\n",
      "Input and Output processed\n",
      "Model created\n",
      "Training in progress\n",
      "Epoch 1/50\n",
      "3791/3791 [==============================] - 1260s 332ms/step - loss: 4.8048\n",
      "Epoch 2/50\n",
      "3791/3791 [==============================] - 1062s 280ms/step - loss: 4.6709\n",
      "Epoch 3/50\n",
      "3791/3791 [==============================] - 785s 207ms/step - loss: 4.5194\n",
      "Epoch 4/50\n",
      "3791/3791 [==============================] - 775s 205ms/step - loss: 4.3471\n",
      "Epoch 5/50\n",
      "3791/3791 [==============================] - 782s 206ms/step - loss: 4.1412\n",
      "Epoch 6/50\n",
      "3791/3791 [==============================] - 778s 205ms/step - loss: 3.8927\n",
      "Epoch 7/50\n",
      "3791/3791 [==============================] - 775s 204ms/step - loss: 3.5939\n",
      "Epoch 8/50\n",
      "3791/3791 [==============================] - 784s 207ms/step - loss: 3.2749s - \n",
      "Epoch 9/50\n",
      "3791/3791 [==============================] - 809s 213ms/step - loss: 2.9695\n",
      "Epoch 10/50\n",
      "3791/3791 [==============================] - 1284s 339ms/step - loss: 2.6963\n",
      "Epoch 11/50\n",
      "3791/3791 [==============================] - 1109s 292ms/step - loss: 2.4649\n",
      "Epoch 12/50\n",
      "3791/3791 [==============================] - 754s 199ms/step - loss: 2.2757\n",
      "Epoch 13/50\n",
      "3791/3791 [==============================] - 749s 198ms/step - loss: 2.1137\n",
      "Epoch 14/50\n",
      "3791/3791 [==============================] - 966s 255ms/step - loss: 1.9812\n",
      "Epoch 15/50\n",
      "3791/3791 [==============================] - 1244s 328ms/step - loss: 1.8737\n",
      "Epoch 16/50\n",
      "3791/3791 [==============================] - 1238s 327ms/step - loss: 1.7757\n",
      "Epoch 17/50\n",
      "3791/3791 [==============================] - 1270s 335ms/step - loss: 1.6936\n",
      "Epoch 18/50\n",
      "3791/3791 [==============================] - 1219s 322ms/step - loss: 1.6127\n",
      "Epoch 19/50\n",
      "3791/3791 [==============================] - 1186s 313ms/step - loss: 1.5488\n",
      "Epoch 20/50\n",
      "3791/3791 [==============================] - 1250s 330ms/step - loss: 1.4840\n",
      "Epoch 21/50\n",
      "3791/3791 [==============================] - 822s 217ms/step - loss: 1.4340\n",
      "Epoch 22/50\n",
      "3791/3791 [==============================] - 756s 199ms/step - loss: 1.3860\n",
      "Epoch 23/50\n",
      "3791/3791 [==============================] - 761s 201ms/step - loss: 1.3341\n",
      "Epoch 24/50\n",
      "3791/3791 [==============================] - 769s 203ms/step - loss: 1.2955\n",
      "Epoch 25/50\n",
      "3791/3791 [==============================] - 765s 202ms/step - loss: 1.2493\n",
      "Epoch 26/50\n",
      "3791/3791 [==============================] - 828s 218ms/step - loss: 1.2165\n",
      "Epoch 27/50\n",
      "3791/3791 [==============================] - 854s 225ms/step - loss: 1.1795\n",
      "Epoch 28/50\n",
      "3791/3791 [==============================] - 796s 210ms/step - loss: 1.1488\n",
      "Epoch 29/50\n",
      "3791/3791 [==============================] - 776s 205ms/step - loss: 1.1223\n",
      "Epoch 30/50\n",
      "3791/3791 [==============================] - 771s 203ms/step - loss: 1.0878\n",
      "Epoch 31/50\n",
      "3791/3791 [==============================] - 771s 203ms/step - loss: 1.0619\n",
      "Epoch 32/50\n",
      "3791/3791 [==============================] - 766s 202ms/step - loss: 1.0385\n",
      "Epoch 33/50\n",
      "3791/3791 [==============================] - 765s 202ms/step - loss: 1.0154\n",
      "Epoch 34/50\n",
      "3791/3791 [==============================] - 766s 202ms/step - loss: 0.9838\n",
      "Epoch 35/50\n",
      "3791/3791 [==============================] - 768s 203ms/step - loss: 0.9723\n",
      "Epoch 36/50\n",
      "3791/3791 [==============================] - 767s 202ms/step - loss: 0.9477s -\n",
      "Epoch 37/50\n",
      "3791/3791 [==============================] - 773s 204ms/step - loss: 0.9209\n",
      "Epoch 38/50\n",
      "3791/3791 [==============================] - 746s 197ms/step - loss: 0.9036\n",
      "Epoch 39/50\n",
      "3791/3791 [==============================] - 749s 197ms/step - loss: 0.8876s - loss: 0\n",
      "Epoch 40/50\n",
      "3791/3791 [==============================] - 773s 204ms/step - loss: 0.8634\n",
      "Epoch 41/50\n",
      "3791/3791 [==============================] - 777s 205ms/step - loss: 0.8537\n",
      "Epoch 42/50\n",
      "3791/3791 [==============================] - 780s 206ms/step - loss: 0.8344\n",
      "Epoch 43/50\n",
      "3791/3791 [==============================] - 779s 206ms/step - loss: 0.8186\n",
      "Epoch 44/50\n",
      "3791/3791 [==============================] - 795s 210ms/step - loss: 0.7989\n",
      "Epoch 45/50\n",
      "3791/3791 [==============================] - 787s 208ms/step - loss: 0.7896\n",
      "Epoch 46/50\n",
      "3791/3791 [==============================] - 807s 213ms/step - loss: 0.7744\n",
      "Epoch 47/50\n",
      "3791/3791 [==============================] - 786s 207ms/step - loss: 0.7594\n",
      "Epoch 48/50\n",
      "3791/3791 [==============================] - 783s 206ms/step - loss: 0.7423\n",
      "Epoch 49/50\n",
      "3791/3791 [==============================] - 786s 207ms/step - loss: 0.7304\n",
      "Epoch 50/50\n",
      "3791/3791 [==============================] - 1033s 272ms/step - loss: 0.7181\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    \"\"\" Generate a piano midi file \"\"\"\n",
    "    #load the notes used to train the model\n",
    "    with open('C://Users//ishik//Desktop//Data//notes', 'rb') as filepath:\n",
    "        notes = pickle.load(filepath)\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    # Get all pitch names\n",
    "    n_vocab = len(set(notes))\n",
    "    \n",
    "    print('Initiating music generation process.......')\n",
    "    \n",
    "    network_input = get_inputSequences(notes, pitchnames, n_vocab)\n",
    "    normalized_input = network_input / float(n_vocab)\n",
    "    model = create_network(normalized_input, n_vocab)\n",
    "    print('Loading Model weights.....')\n",
    "    model.load_weights('weights.best.music3.hdf5')\n",
    "    print('Model Loaded')\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "    create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputSequences(notes, pitchnames, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    # map between notes and integers and back\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    sequence_length = 100\n",
    "    network_input = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    \n",
    "    network_input = np.reshape(network_input, (len(network_input), 100, 1))\n",
    "    \n",
    "    return (network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # Pick a random integer\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    pattern =list(network_input[start])\n",
    "    prediction_output = []\n",
    "    \n",
    "    print('Generating notes........')\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "        prediction_input=np.asarray(prediction_input).astype(np.float32)\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "        \n",
    "        # Predicted output is the argmax(P(h|D))\n",
    "        index = np.argmax(prediction)\n",
    "        # Mapping the predicted interger back to the corresponding note\n",
    "        result = int_to_note[index]\n",
    "        # Storing the predicted output\n",
    "        prediction_output.append(result)\n",
    "        pattern.append(index)\n",
    "        # Next input to the model\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    print('Notes Generated...')\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    \n",
    "    print('Saving Output file as midi....')\n",
    "\n",
    "    midi_stream.write('midi', fp='test_output4.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating music generation process.......\n",
      "Loading Model weights.....\n",
      "Model Loaded\n",
      "Generating notes........\n",
      "Notes Generated...\n",
      "Saving Output file as midi....\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
